# Problem2_for_Cihlab_entry_2025_COMBMacro
This repository contains the solution to Problem 2, showcasing the screenshots and document for the given task. The screenshots are arranged in the order from 1 to 18, and the version of the response is as follows.

# **Advantages of the COMB Macro in COMB-MCM over General Compute-in-Memory Macros**

The increasing proliferation of Artificial Intelligence (AI) and Machine Learning (ML) applications in edge computing environments has created a significant demand for highly energy-efficient hardware solutions.1 Unlike cloud-based computing, edge devices operate under stringent power limitations, making energy efficiency a critical factor in their design and deployment.1 Traditional von Neumann computing architectures, which maintain a physical separation between processing and memory units, struggle to meet these demanding energy requirements, leading to the well-recognized "memory wall" problem.2 This bottleneck arises because the energy and time consumed in transferring data between the processor and memory become dominant, especially for the data-intensive workloads characteristic of AI. In response to these challenges, Compute-in-Memory (CIM) has emerged as a promising paradigm that seeks to overcome the memory wall by integrating computation directly within or in close proximity to memory arrays, thus minimizing data movement and enhancing both energy efficiency and processing speed.1 Various implementations of CIM leverage different memory technologies, including Static Random-Access Memory (SRAM) and emerging Non-Volatile Memories (NVMs).1 The growing prevalence of AI at the edge necessitates a shift towards such innovative architectures that can deliver high performance under tight power constraints.

The COMB-MCM architecture, featuring the COMB (Computing-on-Memory Boundary) macro, represents a significant advancement in the field of memory-centric computing, specifically designed to enhance the efficiency of CIM for edge machine learning applications.1 This architecture was introduced by researchers at Fudan University at the International Solid-State Circuits Conference (ISSCC) in 2022, a leading forum for advancements in integrated circuits and systems-on-a-chip.11 A key innovation of COMB-MCM is its utilization of a Multi-Chiplet Module (MCM) design, where multiple smaller integrated circuit dies (chiplets) are integrated into a single package.1 This contrasts with the traditional approach of implementing an entire system on a single monolithic System-on-Chip (SoC). The adoption of a chiplet-based architecture offers considerable flexibility in design and manufacturing.1 This report will delve into the specific reasons why the COMB macro within the COMB-MCM architecture offers notable advantages over general CIM macros, particularly when considering the unique demands of edge machine learning. By examining the architectural innovations and performance benefits, a comprehensive understanding of the superiority of this approach can be established.

While Compute-in-Memory (CIM) aims to mitigate the limitations of traditional computing architectures by bringing computation closer to data, many general CIM implementations still grapple with inherent challenges.1 Despite the integration of computation and memory, a degree of data transfer often remains necessary between the memory array, where the primary computations occur, and peripheral circuits responsible for tasks such as accumulation, applying activation functions, and managing output.1 This residual data movement can still create a bottleneck, limiting the potential improvements in both performance and energy efficiency that CIM promises.2 The "memory wall," the widening gap between the speed of processors and the speed at which they can access data, persists as a significant obstacle even for many CIM systems.3 Fetching the necessary operands, including both weights and input activations, to the computational units within the memory array can become a major bottleneck, particularly when dealing with the large parameter sets characteristic of modern deep learning models.1

The energy consumed in moving data within a computing system, especially for the large datasets involved in AI workloads, represents a substantial portion of the total power budget.1 In fact, the energy cost associated with data transfer can often surpass the energy required for the actual arithmetic operations by several orders of magnitude. This is a critical concern for edge devices that operate on limited power resources, as it can restrict the complexity and operational duration of AI applications.1 Furthermore, the latency introduced by these data transfers can significantly impede the real-time performance that many edge AI applications demand.1 Delays in accessing data can directly impact the responsiveness and overall effectiveness of these time-sensitive applications. Therefore, minimizing both the energy and latency overheads associated with data movement is paramount for efficient edge AI hardware.

Many machine learning tasks, including the training and fine-tuning of models, necessitate frequent updates to the model's weights.1 For large neural networks, these weight updates can involve transferring vast amounts of data between the on-chip CIM macro and off-chip memory, such as DRAM.1 This extensive data transfer can severely degrade overall system performance and dramatically increase energy consumption, potentially negating the inherent benefits of performing computations within memory.1 Even in inference-only scenarios, where weight updates are less frequent, the initial loading of the model's weights from off-chip memory into the CIM macro can still contribute significantly to the system's power consumption and introduce latency.1 Efficiently managing weight updates, therefore, remains a significant challenge for achieving optimal performance and energy efficiency in AI hardware, particularly within the constraints of edge computing.

Sparsity, a common characteristic of many efficient neural network models, refers to the presence of a significant number of zero-valued weights and activations.1 Exploiting this inherent sparsity by skipping computations involving zero values offers a substantial opportunity to reduce energy consumption and improve performance in AI accelerators.1 However, general CIM macros often encounter difficulties in efficiently handling fine-grained or arbitrary sparsity patterns.1 Some approaches might necessitate the inclusion of additional hardware components, such as explicit zero-detection blocks, which can introduce overhead in terms of chip area and power consumption.1 Structured and coarse-grained sparsity optimization techniques, which involve zeroing out entire blocks or larger segments of data, are often algorithm-dependent and may not be universally effective across different types of neural networks or sparsity distributions.1 Consequently, achieving optimal power savings and performance gains by effectively leveraging arbitrary sparsity in general CIM implementations remains a considerable challenge.

The COMB architecture, a central component of the COMB-MCM system, was specifically conceived to address the limitations of both traditional in-memory and near-memory computing for neural network processing in edge devices.1 It represents a strategic middle ground between these two dominant memory-centric computing paradigms.1 Unlike conventional in-memory computing, where arithmetic operations are typically performed directly within the memory array often leveraging the analog characteristics of memory cells, and unlike near-memory computing, which employs dedicated processing units placed in close spatial proximity to memory banks, COMB uniquely focuses on performing computation "on the boundary" of the memory.1 This innovative approach likely involves a design where processing elements are tightly integrated with the memory banks within each chiplet, but perhaps not directly within the memory cells themselves. This close proximity is crucial for minimizing the distance that data must traverse for the initial stages of computation, thereby significantly reducing the energy expenditure associated with data movement.1 The term "boundary" might also suggest a more direct and efficient interface mechanism between the memory and the computational units compared to more conventional CIM architectures.

A defining feature of the COMB-MCM architecture is its employment of multiple chiplet modules (MCM) instead of relying on a single, large monolithic System-on-Chip (SoC).1 This involves the integration of several smaller, independently manufactured integrated circuit dies (chiplets) into a single package through the use of advanced packaging technologies, such as 2.5D IC packaging, which was instrumental in validating the feasibility of the COMB-MCM system.11 This modular design philosophy offers significant advantages in terms of design flexibility, as different chiplets within the MCM can be specialized to perform specific functions, including memory storage, data processing, or input/output operations. The chiplets are interconnected via high-bandwidth communication pathways, enabling efficient data exchange and collaboration within the package.1 A key benefit of the MCM approach is its inherent scalability. The total computational and memory resources of the COMB-MCM system can be readily increased by simply incorporating a greater number of chiplets within the module. This allows the same fundamental architecture to be easily adapted to support a wide spectrum of AI tasks characterized by varying computational and memory demands.11 Furthermore, this modularity helps to mitigate the high non-recurring engineering (NRE) costs that are typically associated with designing a unique monolithic SoC for every specific application.14

By concentrating computation at the memory boundary within each chiplet 1, the COMB architecture effectively positions COMB-MCM as an intermediate approach between the extremes of traditional in-memory and near-memory computing paradigms.1 By situating processing units much closer to the memory compared to more traditional CIM processors (which might employ more centralized processing units serving larger, more distant memory banks), COMB-MCM aims to substantially reduce the communication overheads, both in terms of energy consumption and latency, associated with data transfer.1 Simultaneously, by not performing computations directly within the memory cells themselves, as is the case in some analog CIM implementations, COMB likely retains the inherent flexibility and potentially higher precision offered by digital processing units that are located in very close proximity to the memory. This strategic design choice allows COMB-MCM to leverage some of the primary benefits of in-memory computation, such as reduced data movement, while also providing more robust and versatile computational capabilities than might be practically achievable with purely in-memory approaches. This balanced approach is likely key to optimizing performance for the specific requirements of edge machine learning applications.

The most significant advantage of the COMB macro is its ability to substantially enhance energy efficiency by minimizing the movement of data.1 By performing the initial computational steps at the interface of the SRAM memory within each chiplet, the distance that data must travel is significantly reduced when compared to general CIM macros, where processing units might be located at a greater distance from the main memory banks.1 This reduction in data transfer directly translates into lower energy consumption, a crucial attribute for edge devices that often operate under strict power budgets.1 This addresses a key limitation of many general CIM macros, which, despite their aim to reduce data movement compared to traditional CPU/GPU architectures, can still involve considerable data transfer between the memory array and peripheral compute units for tasks like accumulation and other operations, leading to significant energy expenditures.1 The tight integration of compute units with the memory, facilitated by the "memory boundary" approach, is a fundamental architectural strength that yields tangible energy savings, making COMB particularly well-suited for the demanding energy constraints of edge computing.

The integration of the COMB macro within a Multi-Chiplet Module (MCM) design provides a considerable advantage in terms of scalability when compared to general CIM macros, which are typically implemented on a single monolithic die.1 COMB-MCM can readily scale its computational and memory resources by simply increasing the number of chiplets within the package.1 This modularity allows the architecture to be easily adapted to support a wide range of AI tasks with varying levels of complexity and resource requirements.11 General CIM macros, constrained by the practical limitations of fabricating large, complex circuits on a single silicon die, often offer limited scalability. The ability of COMB-MCM to configure the number of chiplets provides a more flexible and cost-effective means of addressing a diverse set of AI applications without the need to design entirely new monolithic chips for each specific use case.14 This scalability is particularly important in the rapidly evolving field of AI, where the computational demands of models are constantly increasing.

By performing computation in close proximity to the memory within each chiplet, the COMB macro significantly reduces its dependence on extensive off-chip weight data transfers, a major bottleneck and a key aspect of the "memory wall" problem in traditional CIM architectures when processing large neural networks.1 The near-memory nature of the COMB architecture helps to keep the necessary data closer to the processing units, thereby reducing the latency and energy associated with accessing data from slower off-chip memory. This approach more effectively mitigates the memory wall issue compared to general CIM macros where the processing might still involve significant data movement to and from off-chip memory or even between different on-chip memory banks and processing units that are not as tightly coupled as in the COMB architecture. By reducing the need to frequently access off-chip memory, COMB-MCM can achieve better performance and energy efficiency for memory-intensive AI workloads.

The COMB macro incorporates a "bipolar bitwise sparsity optimization" technique.1 This suggests a highly efficient mechanism for handling sparse data, whether it be input activations or network weights, at a very granular level – individual bits.14 The term "bipolar" likely indicates its capability to handle both positive and negative sparse values effectively. This bitwise approach potentially allows for more precise skipping of unnecessary computations compared to general CIM macros that might only exploit sparsity at a coarser level, such as skipping entire zero-valued weights or activations. Importantly, the COMB architecture aims to achieve this sparsity optimization without requiring dedicated zero-detection blocks, which can add overhead in terms of chip area and power in some general CIM designs.1 This advanced sparsity optimization can lead to substantial reductions in computational power consumption for AI algorithms that exhibit a high degree of sparsity, a common characteristic of many efficient neural network models.14

The combined effect of the architectural advantages inherent in the COMB macro – reduced data movement, superior scalability through the use of MCM, effective mitigation of the memory wall, and advanced sparsity optimization – results in a lower overall system power overhead for COMB-MCM when compared to general CIM macros.1 Each of these factors contributes to minimizing energy consumption at various levels of the system's operation. This lower power overhead makes COMB-MCM a particularly compelling solution for power-constrained edge computing applications where battery life or thermal management are critical design considerations.1 The holistic optimization of various aspects of the architecture leads to a more energy-efficient system overall.

The research material indicates that the COMB-MCM architecture achieves high macro computing energy efficiency and low system power overhead.1 Notably, a near-memory computing system presented by Alibaba and Fudan University at ISSCC 2022, which is very likely the COMB-MCM, demonstrated an energy efficiency of 32.9 TOPS/W while performing computations with INT3 precision.1 This figure highlights the significant energy efficiency achieved by the architecture, making it highly suitable for edge applications with limited power resources. Furthermore, the energy efficiency of COMB-MCM is reported to improve with input sparsity, reaching 46.4 TOPS/W at 50% input sparsity and potentially exceeding this at 90% input sparsity.1 This showcases the effectiveness of the integrated bipolar bitwise sparsity optimization technique in further enhancing the architecture's efficiency for sparse AI workloads. These reported performance metrics underscore the energy-saving potential of the COMB macro.

A review of contemporary edge processors discussed in the research material 1 concluded that Processing in-Memory (PIM) architectures, which include CIM, generally exhibit significant energy efficiency and consume less power compared to other architectural approaches such as dataflow and neuromorphic processors. This positions CIM, and consequently COMB-MCM, as a promising direction for developing energy-efficient solutions for edge computing. Another digital NVM-based CIM macro (DNV-CIM) achieved an energy efficiency of up to 39.9 TOPS/W when running a 4-bit quantized ResNet18 on the CIFAR-10 dataset.1 While this performance is comparable to that of COMB-MCM, the fundamental differences in memory technology (NVM versus SRAM) and the unique "memory boundary" computing approach of COMB-MCM suggest potential trade-offs in other aspects, such as data retention versus access speed. Other SRAM-based CIM macros have also been reported with varying energy efficiencies.9 For instance, one such macro achieved an impressive 687.5 TOPS/W 19, although the specific operating conditions and precision for this result are not immediately available for a direct comparison. Another SRAM-based near-memory computing architecture achieved an energy efficiency of 32.9 TOPS/W with INT3 precision 1, which is similar to the performance of COMB-MCM, suggesting that the memory boundary approach in COMB-MCM is a significant factor in achieving this level of efficiency. Based on the available information, the energy efficiency of COMB-MCM appears to be competitive with or superior to many other state-of-the-art edge processors and CIM architectures, highlighting its potential for energy-efficient AI acceleration at the edge.

The COMB-MCM chip has been implemented and evaluated using both 65nm and 28nm process technologies.11 The initial feasibility and functionality of the MCM system incorporating the COMB architecture were successfully demonstrated using the 65nm process. Furthermore, the research team was able to achieve even better performance metrics when the design was implemented using the more advanced 28nm process technology.11 The fact that COMB-MCM has been realized and tested in two different process nodes indicates the robustness and potential for optimization of the architecture. The improved performance observed in the 28nm process is consistent with the general trend in semiconductor technology, where smaller feature sizes and lower operating voltages associated with more advanced process nodes typically lead to enhanced performance and reduced power consumption. This also suggests that future iterations of the COMB-MCM architecture implemented on even more advanced process nodes could potentially yield further improvements in both performance and energy efficiency.

In conclusion, the COMB macro within the COMB-MCM architecture presents several significant advantages over general Compute-in-Memory (CIM) macros, making it a highly promising solution for achieving energy-efficient machine learning at the edge. These advantages are primarily attributed to its innovative Computing-on-Memory Boundary (COMB) approach and the strategic utilization of Multi-Chiplet Modules (MCM). The key benefits include substantially enhanced energy efficiency resulting from the minimization of data movement through computation at the memory boundary, superior scalability facilitated by the modular MCM design, effective mitigation of the memory wall issue by keeping computation in close proximity to the data, advanced bipolar bitwise sparsity optimization enabling finer-grained power savings, and an overall lower system power overhead, all of which are crucial for the resource-constrained nature of edge computing environments. The reported energy efficiency figures for COMB-MCM, particularly the 32.9 TOPS/W at INT3 precision and the further improvements observed with increasing sparsity, strongly support its potential for energy-efficient AI acceleration. Furthermore, its competitive performance compared to other state-of-the-art edge processors and CIM architectures, along with its successful implementation in multiple process nodes, underscores its viability and potential for future advancements. The COMB-MCM architecture represents a significant step forward in addressing the growing demand for energy-efficient and scalable hardware solutions for deploying increasingly complex AI models at the edge, paving the way for more intelligent and autonomous edge devices.

#### **Works cited**

1. COMB-MCM: Computing-on-Memory-Boundary NN Processor with Bipolar Bitwise Sparsity Optimization for Scalable Multi-Chiplet-Module Edge Machine Learning | Request PDF \- ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/publication/359313701\_COMB-MCM\_Computing-on-Memory-Boundary\_NN\_Processor\_with\_Bipolar\_Bitwise\_Sparsity\_Optimization\_for\_Scalable\_Multi-Chiplet-Module\_Edge\_Machine\_Learning](https://www.researchgate.net/publication/359313701_COMB-MCM_Computing-on-Memory-Boundary_NN_Processor_with_Bipolar_Bitwise_Sparsity_Optimization_for_Scalable_Multi-Chiplet-Module_Edge_Machine_Learning)  
2. BECAUSE MEMORY MATTERS \- Vsquared Ventures, accessed May 7, 2025, [https://vsquared.vc/news/because-memory-matters/](https://vsquared.vc/news/because-memory-matters/)  
3. An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference \- arXiv, accessed May 7, 2025, [https://arxiv.org/html/2406.08413v1](https://arxiv.org/html/2406.08413v1)  
4. IMPACT: In-Memory ComPuting Architecture based on Y-FlAsh Technology for Coalesced Tsetlin machine inference | Philosophical Transactions of the Royal Society A \- Journals, accessed May 7, 2025, [https://royalsocietypublishing.org/doi/10.1098/rsta.2023.0393](https://royalsocietypublishing.org/doi/10.1098/rsta.2023.0393)  
5. A Review of SRAM-based Compute-in-Memory Circuits \- arXiv, accessed May 7, 2025, [https://arxiv.org/html/2411.06079v2](https://arxiv.org/html/2411.06079v2)  
6. What are the limitations of traditional CPU architecture in terms of memory bandwidth and access latency? \- Massed Compute, accessed May 7, 2025, [https://massedcompute.com/faq-answers/?question=What%20are%20the%20limitations%20of%20traditional%20CPU%20architecture%20in%20terms%20of%20memory%20bandwidth%20and%20access%20latency?](https://massedcompute.com/faq-answers/?question=What+are+the+limitations+of+traditional+CPU+architecture+in+terms+of+memory+bandwidth+and+access+latency?)  
7. A Survey of Computing-in-Memory Processor: From Circuit to Application \- ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/publication/376781477\_A\_Survey\_of\_Computing-in-Memory\_Processor\_From\_Circuit\_to\_Application](https://www.researchgate.net/publication/376781477_A_Survey_of_Computing-in-Memory_Processor_From_Circuit_to_Application)  
8. Compute-in-Memory Computational Devices \- GSI Technology, accessed May 7, 2025, [https://gsitechnology.com/compute-in-memory-computational-devices/](https://gsitechnology.com/compute-in-memory-computational-devices/)  
9. (PDF) Benchmarking In-Memory Computing Architectures \- ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/publication/366144178\_Benchmarking\_In-memory\_Computing\_Architectures](https://www.researchgate.net/publication/366144178_Benchmarking_In-memory_Computing_Architectures)  
10. Simulation of a Fully Digital Computing-in-Memory for Non-Volatile Memory for Artificial Intelligence Edge Applications \- PMC, accessed May 7, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10301468/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10301468/)  
11. Shanghai-based Fudan University releases a concept Computing-In-Memory AI chip \- ijiwei, accessed May 7, 2025, [https://jw.ijiwei.com/n/809941](https://jw.ijiwei.com/n/809941)  
12. ISSCC 2022 Advance Program 2-3-2022 \- Squarespace, accessed May 7, 2025, [https://static1.squarespace.com/static/6130ef779c7a2574bd4b8888/t/66db0f34cefda9125669e357/1725632309244/ISSCC2022AdvanceProgram+2-3-2022.pdf](https://static1.squarespace.com/static/6130ef779c7a2574bd4b8888/t/66db0f34cefda9125669e357/1725632309244/ISSCC2022AdvanceProgram+2-3-2022.pdf)  
13. About \- Haozhe's Blog, accessed May 7, 2025, [https://zhutmost.com/about](https://zhutmost.com/about)  
14. The research results of Fudan University deposit and calculation fusion artificial intelligence chip COMB-MCM are unveiled in ISSCC. \- News, accessed May 7, 2025, [https://news.metal.com/newscontent/101766247/the-research-results-of-fudan-university-deposit-and-calculation-fusion-artificial-intelligence-chip-comb-mcm-are-unveiled-in-isscc](https://news.metal.com/newscontent/101766247/the-research-results-of-fudan-university-deposit-and-calculation-fusion-artificial-intelligence-chip-comb-mcm-are-unveiled-in-isscc)  
15. Yunzhengmao Wang's research works | Fudan University and other places \- ResearchGate, accessed May 7, 2025, [https://www.researchgate.net/scientific-contributions/Yunzhengmao-Wang-2196873050](https://www.researchgate.net/scientific-contributions/Yunzhengmao-Wang-2196873050)  
16. In Memory Compute: Transforming Data Processing for Speed and Scalability \- Wevolver, accessed May 7, 2025, [https://www.wevolver.com/article/in-memory-compute-transforming-data-processing-for-speed-and-scalability](https://www.wevolver.com/article/in-memory-compute-transforming-data-processing-for-speed-and-scalability)  
17. ‪tianchan guan‬ \- ‪Google Scholar‬, accessed May 7, 2025, [https://scholar.google.no/citations?user=GKq9dNMAAAAJ\&hl=ja](https://scholar.google.no/citations?user=GKq9dNMAAAAJ&hl=ja)  
18. A General-Purpose Compute-in-Memory Processor Combining CPU and Deep Learning with State-of-the-art CPU Efficiency and Enhanced \- Northwestern University, accessed May 7, 2025, [http://nu-vlsi.eecs.northwestern.edu/CIM\_CPU\_VLSI2023.pdf](http://nu-vlsi.eecs.northwestern.edu/CIM_CPU_VLSI2023.pdf)  
19. Literature on SRAM-based Compute-In-Memory \- GitHub, accessed May 7, 2025, [https://github.com/BUAA-CI-LAB/Literatures-on-SRAM-based-CIM](https://github.com/BUAA-CI-LAB/Literatures-on-SRAM-based-CIM)
